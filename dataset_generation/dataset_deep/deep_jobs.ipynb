{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        timestamp  bytesRead  bytesWritten\n",
      "0   1687960785000       10.0           5.0\n",
      "1   1687960790000       20.0          15.0\n",
      "2   1687960795000       30.0          25.0\n",
      "3   1687960800000        0.0           0.0\n",
      "4   1687960805000        0.0           0.0\n",
      "..            ...        ...           ...\n",
      "70  1687961135000        0.0           0.0\n",
      "71  1687961140000        0.0           0.0\n",
      "72  1687961145000       40.0          35.0\n",
      "73  1687961150000       50.0          45.0\n",
      "74  1687961155000       60.0          55.0\n",
      "\n",
      "[75 rows x 3 columns]\n",
      "{'timestamp': [1687960785000, 1687960790000, 1687960795000, 1687960800000, 1687960805000, 1687960810000, 1687960815000, 1687960820000, 1687960825000, 1687960830000, 1687960835000, 1687960840000, 1687960845000, 1687960850000, 1687960855000, 1687960860000, 1687960865000, 1687960870000, 1687960875000, 1687960880000, 1687960885000, 1687960890000, 1687960895000, 1687960900000, 1687960905000, 1687960910000, 1687960915000, 1687960920000, 1687960925000, 1687960930000, 1687960935000, 1687960940000, 1687960945000, 1687960950000, 1687960955000, 1687960960000, 1687960965000, 1687960970000, 1687960975000, 1687960980000, 1687960985000, 1687960990000, 1687960995000, 1687961000000, 1687961005000, 1687961010000, 1687961015000, 1687961020000, 1687961025000, 1687961030000, 1687961035000, 1687961040000, 1687961045000, 1687961050000, 1687961055000, 1687961060000, 1687961065000, 1687961070000, 1687961075000, 1687961080000, 1687961085000, 1687961090000, 1687961095000, 1687961100000, 1687961105000, 1687961110000, 1687961115000, 1687961120000, 1687961125000, 1687961130000, 1687961135000, 1687961140000, 1687961145000, 1687961150000, 1687961155000], 'bytesRead': [10.0, 20.0, 30.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 40.0, 50.0, 60.0], 'bytesWritten': [5.0, 15.0, 25.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 35.0, 45.0, 55.0]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class WorkflowSynthesizer:\n",
    "    \"\"\"\n",
    "    Class for synthesizing a workflow from a dictionary of jobs. \n",
    "    The jobs and the workflow are represented as dataframes with time-series data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of the WorkflowSynthesizer class.\n",
    "        \"\"\"\n",
    "        self.workflow = pd.DataFrame()\n",
    "\n",
    "    def synthetize(self, data_for_jobs):\n",
    "        \"\"\"\n",
    "        Synthesize a workflow from the provided jobs.\n",
    "\n",
    "        Args:\n",
    "            data_for_jobs (dict): A dictionary where keys are job names \n",
    "            and values are DataFrames with columns 'timestamp', 'bytesRead', \n",
    "            and 'bytesWritten'.\n",
    "        \"\"\"\n",
    "        min_timestamp = min(df['timestamp'].min() for df in data_for_jobs.values())\n",
    "        max_timestamp = max(df['timestamp'].max() for df in data_for_jobs.values())\n",
    "        \n",
    "        # Create a DataFrame with a uniform timestamp range\n",
    "        synthetic_timestamps = np.arange(min_timestamp, max_timestamp + 1, 5000)\n",
    "        synthetic_df = pd.DataFrame({'timestamp': synthetic_timestamps})\n",
    "        \n",
    "        for job_name, job_df in data_for_jobs.items():\n",
    "            # Merge with the synthetic DataFrame to get uniform timestamps\n",
    "            merged_df = pd.merge(synthetic_df, job_df, on='timestamp', how='left').fillna(0)\n",
    "            \n",
    "            # Sum up the bytesRead and bytesWritten across all jobs\n",
    "            if self.workflow.empty:\n",
    "                self.workflow = merged_df\n",
    "            else:\n",
    "                self.workflow['bytesRead'] += merged_df['bytesRead']\n",
    "                self.workflow['bytesWritten'] += merged_df['bytesWritten']\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"\n",
    "        Convert the synthesized workflow to a dictionary format.\n",
    "\n",
    "        Returns:\n",
    "            dict: The synthesized workflow in dictionary format.\n",
    "        \"\"\"\n",
    "        output = {}\n",
    "        output['timestamp'] = self.workflow['timestamp'].tolist()\n",
    "        output['bytesRead'] = self.workflow['bytesRead'].tolist()\n",
    "        output['bytesWritten'] = self.workflow['bytesWritten'].tolist()\n",
    "        return output\n",
    "    \n",
    "    def plot_workflow(self):\n",
    "        \"\"\"\n",
    "        Plot the time series of bytesRead and bytesWritten for the synthesized workflow.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(self.workflow['timestamp'], self.workflow['bytesRead'], label='Bytes Read')\n",
    "        plt.plot(self.workflow['timestamp'], self.workflow['bytesWritten'], label='Bytes Written')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('Bytes')\n",
    "        plt.title('Bytes Read and Written Over Time')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "data_for_jobs = {\n",
    "    'job1': pd.DataFrame({'timestamp': [1687960785000, 1687960790000, 1687960795000], 'bytesRead': [10, 20, 30], 'bytesWritten': [5, 15, 25]}),\n",
    "    'job2': pd.DataFrame({'timestamp': [1687961145000, 1687961150000, 1687961155000], 'bytesRead': [40, 50, 60], 'bytesWritten': [35, 45, 55]})\n",
    "}\n",
    "\n",
    "synthesizer = WorkflowSynthesizer()\n",
    "synthesizer.synthetize(data_for_jobs)\n",
    "print(synthesizer.workflow)\n",
    "print(synthesizer.to_dict())\n",
    "#synthesizer.plot_workflow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For accessPatternRead.json, the columns are: ['timestamp', 'access_pattern_read_random', 'access_pattern_read_sequential', 'access_pattern_read_stride', 'access_pattern_read_unclassified']\n",
      "For ioSizesWrite.json, the columns are: ['timestamp', 'io_sizes_write_0B_16B', 'io_sizes_write_16B_4KB', 'io_sizes_write_4KB_128KB', 'io_sizes_write_128KB_1MB', 'io_sizes_write_1MB_16MB', 'io_sizes_write_16MB_128MB', 'io_sizes_write_128MB_+']\n",
      "For operationsCount.json, the columns are: ['timestamp', 'operations_count_read', 'operations_count_write']\n",
      "For volume.json, the columns are: ['timestamp', 'bytesRead', 'bytesWritten']\n"
     ]
    }
   ],
   "source": [
    "def camel_case_to_snake_case(name):\n",
    "    \"\"\"\n",
    "    Convert a string from CamelCase to snake_case.\n",
    "    \n",
    "    Parameters:\n",
    "        name (str): The string in CamelCase.\n",
    "        \n",
    "    Returns:\n",
    "        str: The string in snake_case.\n",
    "    \"\"\"\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)).lower()\n",
    "\n",
    "# Update the get_column_names function to include the renamed columns using the new prefix strategy\n",
    "def get_column_names(json_file_name):\n",
    "    \"\"\"\n",
    "    Given a JSON file name, this function returns a list of relevant column names.\n",
    "    \n",
    "    Parameters:\n",
    "        json_file_name (str): The name of the JSON file.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of strings representing column names.\n",
    "    \"\"\"\n",
    "    # Extract the prefix from the file name and convert it to snake_case\n",
    "    prefix = camel_case_to_snake_case(json_file_name.split('.')[0])\n",
    "    \n",
    "    column_name_mapping = {\n",
    "        \"accessPatternRead.json\": [\"timestamp\", f\"{prefix}_random\", f\"{prefix}_sequential\", f\"{prefix}_stride\", f\"{prefix}_unclassified\"],\n",
    "        \"accessPatternWrite.json\": [\"timestamp\", f\"{prefix}_random\", f\"{prefix}_sequential\", f\"{prefix}_stride\", f\"{prefix}_unclassified\"],\n",
    "        \"ioSizesRead.json\": [\"timestamp\", f\"{prefix}_0B_16B\", f\"{prefix}_16B_4KB\", f\"{prefix}_4KB_128KB\", f\"{prefix}_128KB_1MB\", f\"{prefix}_1MB_16MB\", f\"{prefix}_16MB_128MB\", f\"{prefix}_128MB_+\"],\n",
    "        \"ioSizesWrite.json\": [\"timestamp\", f\"{prefix}_0B_16B\", f\"{prefix}_16B_4KB\", f\"{prefix}_4KB_128KB\", f\"{prefix}_128KB_1MB\", f\"{prefix}_1MB_16MB\", f\"{prefix}_16MB_128MB\", f\"{prefix}_128MB_+\"],\n",
    "        \"operationsCount.json\": [\"timestamp\", f\"{prefix}_read\", f\"{prefix}_write\"],\n",
    "        \"volume.json\": [\"timestamp\", \"bytesRead\", \"bytesWritten\"]\n",
    "    }\n",
    "    \n",
    "    return column_name_mapping.get(json_file_name, [])\n",
    "\n",
    "# Test the above function\n",
    "test_file_names = [\"accessPatternRead.json\", \"ioSizesWrite.json\", \n",
    "                   \"operationsCount.json\", \"volume.json\"]\n",
    "for name in test_file_names:\n",
    "    print(f\"For {name}, the columns are: {get_column_names(name)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def is_file_extension(filename, expected_extension):\n",
    "    \"\"\"\n",
    "    Check if the file has the expected extension.\n",
    "    \n",
    "    Parameters:\n",
    "        filename (str): The name of the file.\n",
    "        expected_extension (str): The expected file extension (without the dot).\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the file has the expected extension, False otherwise.\n",
    "    \"\"\"\n",
    "    _, file_extension = os.path.splitext(filename)\n",
    "    \n",
    "    return file_extension == f\".{expected_extension}\"\n",
    "\n",
    "# Tests\n",
    "assert is_file_extension(\"example.json\", \"json\") == True\n",
    "assert is_file_extension(\"example.txt\", \"json\") == False\n",
    "assert is_file_extension(\"example.json\", \"txt\") == False\n",
    "assert is_file_extension(\"example.JSON\", \"json\") == False  # Case-sensitive\n",
    "assert is_file_extension(\"example\", \"json\") == False       # No extension\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Processing workflow : ECMWF-649c3c40cc9340246f87cb58...\n",
      "Found job_folder: 371912, is_folder:True\n",
      "job_folder: 371912\n",
      "    Browsing 371912...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371912.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371912.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371912.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371912.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371912.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371912.csv\n",
      "Found job_folder: 371913, is_folder:True\n",
      "job_folder: 371913\n",
      "    Browsing 371913...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371913.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371913.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371913.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371913.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371913.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371913.csv\n",
      "Found job_folder: 371911, is_folder:True\n",
      "job_folder: 371911\n",
      "    Browsing 371911...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371911.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371911.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371911.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371911.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371911.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371911.csv\n",
      "Found job_folder: 371906, is_folder:True\n",
      "job_folder: 371906\n",
      "    Browsing 371906...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371906.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371906.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371906.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371906.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371906.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371906.csv\n",
      "Found job_folder: 371924, is_folder:True\n",
      "job_folder: 371924\n",
      "    Browsing 371924...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371924.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371924.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371924.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371924.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371924.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371924.csv\n",
      "Found job_folder: 371910, is_folder:True\n",
      "job_folder: 371910\n",
      "    Browsing 371910...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371910.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371910.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371910.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371910.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371910.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371910.csv\n",
      "Found job_folder: 371903, is_folder:True\n",
      "job_folder: 371903\n",
      "    Browsing 371903...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371903.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371903.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371903.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371903.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371903.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371903.csv\n",
      "Found job_folder: 371902, is_folder:True\n",
      "job_folder: 371902\n",
      "    Browsing 371902...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371902.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371902.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371902.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371902.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371902.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371902.csv\n",
      "Found job_folder: 371900, is_folder:True\n",
      "job_folder: 371900\n",
      "    Browsing 371900...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371900.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371900.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371900.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371900.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371900.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371900.csv\n",
      "Found job_folder: 371902.csv, is_folder:False\n",
      "Skipping 371902.csv...\n",
      "Found job_folder: 371908, is_folder:True\n",
      "job_folder: 371908\n",
      "    Browsing 371908...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371908.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371908.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371908.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371908.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371908.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371908.csv\n",
      "Found job_folder: 371905, is_folder:True\n",
      "job_folder: 371905\n",
      "    Browsing 371905...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371905.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371905.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371905.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371905.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371905.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371905.csv\n",
      "Found job_folder: 371907, is_folder:True\n",
      "job_folder: 371907\n",
      "    Browsing 371907...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371907.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371907.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371907.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371907.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371907.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371907.csv\n",
      "Found job_folder: 371904, is_folder:True\n",
      "job_folder: 371904\n",
      "    Browsing 371904...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371904.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371904.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371904.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371904.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371904.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371904.csv\n",
      "Found job_folder: 371909, is_folder:True\n",
      "job_folder: 371909\n",
      "    Browsing 371909...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371909.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371909.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371909.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371909.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371909.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371909.csv\n",
      "----\n",
      "Processing workflow : LQCD-64873bafcc9340246f412faf...\n",
      "Found job_folder: 367660, is_folder:True\n",
      "job_folder: 367660\n",
      "    Browsing 367660...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 367660.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 367660.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 367660.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 367660.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 367660.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 367660.csv\n",
      "Found job_folder: 367661, is_folder:True\n",
      "job_folder: 367661\n",
      "    Browsing 367661...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 367661.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 367661.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 367661.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 367661.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 367661.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 367661.csv\n",
      "Found job_folder: 367665, is_folder:True\n",
      "job_folder: 367665\n",
      "    Browsing 367665...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 367665.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 367665.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 367665.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 367665.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 367665.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 367665.csv\n",
      "Found job_folder: 367668, is_folder:True\n",
      "job_folder: 367668\n",
      "    Browsing 367668...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 367668.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 367668.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 367668.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 367668.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 367668.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 367668.csv\n",
      "----\n",
      "Processing workflow : deep_jobs.ipynb...\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to hold the data for each job\n",
    "data_for_jobs = {}\n",
    "\n",
    "# Loop through all items in the current directory\n",
    "for wf_folder in os.listdir():\n",
    "    # Check if the item is a folder and if its name is numeric (i.e., a job number)\n",
    "    print(f\"----\\nProcessing workflow : {wf_folder}...\")\n",
    "    if os.path.isdir(wf_folder):\n",
    "        # Initialize a dictionary to hold the data for this specific job\n",
    "        data_for_this_job = {}\n",
    "        # Loop through all JSON files in this folder\n",
    "        for job_folder in os.listdir(wf_folder):\n",
    "            is_folder = os.path.isdir(os.path.join(os.getcwd(), wf_folder, job_folder))\n",
    "            print(f\"Found job_folder: {job_folder}, is_folder:{is_folder}\")\n",
    "            # Ensure that we are dealing with job folder\n",
    "            if is_folder:\n",
    "                print(f\"job_folder: {job_folder}\")\n",
    "                # Initialize a DataFrame to hold the data for this specific job\n",
    "                df_for_this_job = pd.DataFrame()\n",
    "                print(f\"    Browsing {job_folder}...\")\n",
    "                for json_file in os.listdir(os.path.join(wf_folder, job_folder)):\n",
    "                    if is_file_extension(json_file, \"json\"):\n",
    "                        print(f\"      Processing {json_file}...\")\n",
    "                        # Construct the full path to the JSON file\n",
    "                        json_file_path = os.path.join(os.getcwd(), wf_folder, job_folder, json_file)\n",
    "                        \n",
    "                        # Read the JSON file into a list of lists\n",
    "                        with open(json_file_path, 'r') as f:\n",
    "                            json_data = json.load(f)\n",
    "                        \n",
    "                        # Create a temporary DataFrame from the JSON data\n",
    "                        df_temp = pd.DataFrame(json_data, columns=\n",
    "                                            get_column_names(json_file))\n",
    "                        \n",
    "                        # Merge the temporary DataFrame into the DataFrame for this job, based on the 'timestamp' column\n",
    "                        if df_for_this_job.empty:\n",
    "                            df_for_this_job = df_temp\n",
    "                        else:\n",
    "                            df_for_this_job = pd.merge(df_for_this_job, \n",
    "                                                       df_temp, \n",
    "                                                       on='timestamp', \n",
    "                                                       how='outer')\n",
    "                            \n",
    "                    # Save the DataFrame for this job to a CSV file\n",
    "                    csv_file_path = os.path.join(os.getcwd(), wf_folder, f\"{job_folder}.csv\")\n",
    "                    \n",
    "                    #df_for_this_job.to_csv(csv_file_path, index=False)\n",
    "                    print(f\"            Saving here... {job_folder}.csv\")\n",
    "                    \n",
    "                    # Add this DataFrame to the dictionary\n",
    "                    data_for_jobs[job_folder] = df_for_this_job\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping {job_folder}...\")\n",
    "# synthesizer = WorkflowSynthesizer()\n",
    "# synthesizer.synthetize(data_for_jobs)\n",
    "# print(synthesizer.workflow)\n",
    "# print(synthesizer.to_dict())\n",
    "# synthesizer.plot_workflow()\n",
    "\n",
    "\n",
    "def list_and_classify_directory_contents(directory_path):\n",
    "    \"\"\"\n",
    "    List and classify the contents of a given directory into folders and files.\n",
    "    \n",
    "    Parameters:\n",
    "        directory_path (str): The path to the directory to list.\n",
    "        \n",
    "    Returns:\n",
    "        None: Prints the classification results.\n",
    "    \"\"\"\n",
    "    for item in os.listdir(directory_path):\n",
    "        item_path = os.path.join(directory_path, item)\n",
    "        \n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"{item} -> Folder\")\n",
    "        elif os.path.isfile(item_path):\n",
    "            print(f\"{item} -> File\")\n",
    "        else:\n",
    "            print(f\"{item} -> Unknown\")\n",
    "\n",
    "# Usage example\n",
    "# Replace 'your_directory_path_here' with the path of the directory you want to list and classify.\n",
    "# Uncomment the line below to run the function.\n",
    "# list_and_classify_directory_contents('your_directory_path_here')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import unittest\n",
    "import requests\n",
    "import urllib3\n",
    "from app_decomposer.utils_demo import *\n",
    "\n",
    "from loguru import logger\n",
    "from unittest.mock import patch\n",
    "from pprint import pprint\n",
    "from os.path import dirname, abspath\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from performance_data.data_table import PhaseData, DataTable\n",
    "from performance_data import DATASET_FILE\n",
    "from app_decomposer import DEFAULT_CONFIGURATION, KIWI_CONFIG, CURRENT_DIR, API_DICT_TS, IOI_SAMPLING_PERIOD, DATASET_SOURCE\n",
    "from app_decomposer.api_connector import request_delegator\n",
    "from app_decomposer.config_parser import Configuration\n",
    "from app_decomposer.api_connector import TimeSeries\n",
    "from cluster_simulator.analytics import *\n",
    "from loguru import logger\n",
    "import simpy\n",
    "from loguru import logger\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from cluster_simulator.utils import convex_hull\n",
    "from cluster_simulator.cluster import Cluster, Tier, EphemeralTier, bandwidth_share_model, compute_share_model, get_tier, convert_size\n",
    "from cluster_simulator.phase import DelayPhase, ComputePhase, IOPhase\n",
    "from cluster_simulator.application import Application\n",
    "from cluster_simulator.analytics import display_run\n",
    "from cluster_simulator.ephemeral_placement import ClusterBlackBox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ioSizes entry: \n",
      "{'timestamp': array([1687960690000, 1687960695000, 1687960700000, 1687960705000,\n",
      "       1687960710000, 1687960715000, 1687960720000, 1687960725000,\n",
      "       1687960730000, 1687960735000, 1687960740000, 1687960745000,\n",
      "       1687960750000, 1687960755000, 1687960760000, 1687960765000,\n",
      "       1687960770000, 1687960775000])}\n",
      "\n",
      "volume entry: \n",
      "{'timestamp': array([1687960690000, 1687960695000, 1687960700000, 1687960705000,\n",
      "       1687960710000, 1687960715000, 1687960720000, 1687960725000,\n",
      "       1687960730000, 1687960735000, 1687960740000, 1687960745000,\n",
      "       1687960750000, 1687960755000, 1687960760000, 1687960765000,\n",
      "       1687960770000, 1687960775000]), 'bytesRead': array([832678,      0,      0,      0,      0,  35936,    218,   3546,\n",
      "          158,    158,    158,    158,    158,      0,    316,    158,\n",
      "            0,    158]), 'bytesWritten': array([        0,         0,         0,         0,         0,    131072,\n",
      "       872448408, 707461608, 791280104, 770243048, 759822824, 759757288,\n",
      "       837769738, 837704202, 905230892, 905230892, 887546298,  18802800])}\n",
      "\n",
      "operationsCount entry: \n",
      "{'timestamp': array([1687960690000, 1687960695000, 1687960700000, 1687960705000,\n",
      "       1687960710000, 1687960715000, 1687960720000, 1687960725000,\n",
      "       1687960730000, 1687960735000, 1687960740000, 1687960745000,\n",
      "       1687960750000, 1687960755000, 1687960760000, 1687960765000,\n",
      "       1687960770000, 1687960775000]), 'operationRead': array([ 9,  0,  0,  0,  0, 13,  2, 10,  2,  2,  2,  2,  2,  0,  4,  2,  0,\n",
      "        2]), 'operationWrite': array([ 0,  0,  0,  0,  0,  2, 90, 82, 87, 84, 84, 83, 92, 91, 99, 99, 93,\n",
      "       36])}\n",
      "\n",
      "accessPattern entry: \n",
      "{'timestamp': array([1687960690000, 1687960695000, 1687960700000, 1687960705000,\n",
      "       1687960710000, 1687960715000, 1687960720000, 1687960725000,\n",
      "       1687960730000, 1687960735000, 1687960740000, 1687960745000,\n",
      "       1687960750000, 1687960755000, 1687960760000, 1687960765000,\n",
      "       1687960770000, 1687960775000]), 'accessRandWrite': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'accessSeqWrite': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'accessStrWrite': array([ 0,  0,  0,  0,  0,  0, 80, 40, 80, 80, 40, 40, 80, 80, 80, 80, 80,\n",
      "        0]), 'accessUnclWrite': array([ 0,  0,  0,  0,  0,  2, 10, 42,  8,  4, 43, 43, 12, 12, 18, 19, 13,\n",
      "       36]), 'accessRandRead': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'accessSeqRead': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'accessStrRead': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'accessUnclRead': array([ 9,  0,  0,  0,  0, 13,  2, 10,  2,  2,  2,  2,  2,  2,  2,  2,  0,\n",
      "        2])}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The function get_job_timeseries_from_file takes a workflow folder (wf_folder) and a job ID (job_id) as input parameters. It returns a dictionary containing various time series arrays for the specified job.\n",
    "\n",
    "In this function, the job folder path is constructed using os.path.join and checked for existence. If the folder doesn't exist, a FileNotFoundError is raised. The function then initializes a dictionary (timeseries) to hold the time series data.\n",
    "\n",
    "It iterates over all JSON files in the job folder, reading each file into a Pandas DataFrame and removing duplicate timestamps. It then populates the timeseries dictionary with the cleaned data.\n",
    "\n",
    "To generate a CSV file with the specified columns, you'll need to first map the columns from the individual DataFrames to the target columns in the CSV file. Based on the dictionary structures you provided, here's how I interpret the mapping:\n",
    "\n",
    "timestamp: Directly taken from all the entries (common).\n",
    "bytesRead: From the volume entry.\n",
    "bytesWritten: From the volume entry.\n",
    "operationRead: From the operationsCount entry, specifically the operations_count_read field.\n",
    "operationWrite: From the operationsCount entry, specifically the operations_count_write field.\n",
    "accessRandRead: From the accessPatternRead entry, specifically the access_pattern_read_random field.\n",
    "accessSeqRead: From the accessPatternRead entry, specifically the access_pattern_read_sequential field.\n",
    "accessStrRead: From the accessPatternRead entry, specifically the access_pattern_read_stride field.\n",
    "accessUnclRead: From the accessPatternRead entry, specifically the access_pattern_read_unclassified field.\n",
    "accessRandWrite: From the accessPatternWrite entry, specifically the access_pattern_write_random field.\n",
    "accessSeqWrite: From the accessPatternWrite entry, specifically the access_pattern_write_sequential field.\n",
    "accessStrWrite: From the accessPatternWrite entry, specifically the access_pattern_write_stride field.\n",
    "accessUnclWrite: From the accessPatternWrite entry, specifically the access_pattern_write_unclassified field.\n",
    "\n",
    "'''\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_job_timeseries_from_json_file(wf_folder, job_id, skip_columns=[]):\n",
    "    \"\"\"\n",
    "    Method to extract time series data for a specific job within a workflow folder.\n",
    "    \n",
    "    Parameters:\n",
    "        wf_folder (str): The workflow folder containing job data.\n",
    "        job_id (str): The specific job ID to extract time series data for.\n",
    "        skip_columns (list): List of columns to be skipped.\n",
    "    \n",
    "    Returns:\n",
    "        job_timeseries (dict): with 'volume', 'operationsCount' and 'accesspattern' as keys.\n",
    "    \"\"\"\n",
    "    # Construct the path to the job folder\n",
    "    job_folder_path = os.path.join(wf_folder, str(job_id))\n",
    "    \n",
    "    if not os.path.exists(job_folder_path):\n",
    "        raise FileNotFoundError(f\"Job folder for job_id {job_id} not found in {wf_folder}\")\n",
    "\n",
    "    # Initialize a dictionary to hold time series data\n",
    "    job_timeseries = {}\n",
    "\n",
    "    # Mapping of column names to final output names\n",
    "    column_name_map = {\n",
    "        \"bytesRead\": \"bytesRead\",\n",
    "        \"bytesWritten\": \"bytesWritten\",\n",
    "        \"operations_count_read\": \"operationRead\",\n",
    "        \"operations_count_write\": \"operationWrite\",\n",
    "        \"access_pattern_read_random\": \"accessRandRead\",\n",
    "        \"access_pattern_read_sequential\": \"accessSeqRead\",\n",
    "        \"access_pattern_read_stride\": \"accessStrRead\",\n",
    "        \"access_pattern_read_unclassified\": \"accessUnclRead\",\n",
    "        \"access_pattern_write_random\": \"accessRandWrite\",\n",
    "        \"access_pattern_write_sequential\": \"accessSeqWrite\",\n",
    "        \"access_pattern_write_stride\": \"accessStrWrite\",\n",
    "        \"access_pattern_write_unclassified\": \"accessUnclWrite\",\n",
    "    }\n",
    "\n",
    "    # Mapping of JSON file names to output dictionary keys\n",
    "    json_key_map = {\n",
    "        \"volume.json\": \"volume\",\n",
    "        \"operationsCount.json\": \"operationsCount\",\n",
    "        \"accessPatternRead.json\": \"accessPattern\",\n",
    "        \"accessPatternWrite.json\": \"accessPattern\",\n",
    "        \"ioSizesRead.json\": \"ioSizes\",\n",
    "        \"ioSizesWrite.json\": \"ioSizes\"\n",
    "    }\n",
    "\n",
    "    # Loop through all JSON files in this folder\n",
    "    for json_file in os.listdir(job_folder_path):\n",
    "        # Full path to the JSON file\n",
    "        json_file_path = os.path.join(job_folder_path, json_file)\n",
    "        \n",
    "        # Read the JSON file into a DataFrame\n",
    "        df = pd.read_json(json_file_path)\n",
    "        \n",
    "        # Get column names based on the json_file\n",
    "        column_names = get_column_names(json_file)\n",
    "        df.columns = column_names\n",
    "\n",
    "        # Remove duplicate timestamps, if any\n",
    "        df_clean = df.drop_duplicates(subset=['timestamp'])\n",
    "\n",
    "        # Determine the dictionary key corresponding to this JSON file\n",
    "        key = json_key_map.get(json_file)\n",
    "\n",
    "        # Initialize a sub-dictionary for this type of time series\n",
    "        if key not in job_timeseries:\n",
    "            job_timeseries[key] = {}\n",
    "\n",
    "        # Rename columns to their final output names\n",
    "        for column in df_clean.columns:\n",
    "            new_col_name = column_name_map.get(column, column)\n",
    "            if new_col_name not in skip_columns:\n",
    "                job_timeseries[key][new_col_name] = df_clean[column].to_numpy()\n",
    "\n",
    "    return job_timeseries\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "wf_folder = \"/home_nfs/mimounis/iosea-wp3-recommandation-system/dataset_generation/dataset_deep/ECMWF-649c3c40cc9340246f87cb58\"\n",
    "job_id = \"371902\"\n",
    "read_names = get_column_names(\"ioSizesRead.json\")\n",
    "read_names.remove(\"timestamp\")\n",
    "write_names = get_column_names(\"ioSizesWrite.json\")\n",
    "write_names.remove(\"timestamp\")\n",
    "skip_columns = read_names + write_names\n",
    "\n",
    "job_timeseries = get_job_timeseries_from_json_file(wf_folder, job_id,\n",
    "                                                   skip_columns=skip_columns)\n",
    "\n",
    "for ts_key in list(job_timeseries.keys()):\n",
    "    print(f\"\\n{ts_key} entry: \")\n",
    "    print(job_timeseries[ts_key])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "Processing workflow : ECMWF-649c3c40cc9340246f87cb58...\n",
      "Found job_folder: 371912, is_folder:True\n",
      "job_folder: 371912\n",
      "    Browsing 371912...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371912.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371912.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371912.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371912.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371912.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371912.csv\n",
      "---shape of df : 722\n",
      "{'node_count': 1, 'events': [0, 1], 'read_volumes': [0, 3943956138], 'read_bw': [0, 5531495.284712482], 'write_volumes': [0, 66647259], 'write_bw': [0, 93474.41654978962], 'read_pattern': ['Uncl', 'Uncl'], 'write_pattern': ['Uncl', 'Seq'], 'read_operations': [0, 4813], 'write_operations': [0, 411405]}\n",
      "[{'job_id': '371912', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}, {'job_id': '371912', 'nodes': 1, 'read_volume': 3943956138, 'write_volume': 66647259, 'read_io_pattern': 'uncl', 'write_io_pattern': 'seq', 'read_io_size': 819438.2169125286, 'write_io_size': 161.99914682612024, 'ioi_bw': 1124993.9402524545}]\n",
      "Found job_folder: 371913, is_folder:True\n",
      "job_folder: 371913\n",
      "    Browsing 371913...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371913.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371913.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371913.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371913.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371913.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371913.csv\n",
      "---shape of df : 721\n",
      "{'node_count': 1, 'events': [0, 1], 'read_volumes': [0, 4160625527], 'read_bw': [0, 5843575.178370787], 'write_volumes': [0, 70331457], 'write_bw': [0, 98780.13623595505], 'read_pattern': ['Uncl', 'Uncl'], 'write_pattern': ['Uncl', 'Seq'], 'read_operations': [0, 5060], 'write_operations': [0, 434159]}\n",
      "[{'job_id': '371913', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}, {'job_id': '371913', 'nodes': 1, 'read_volume': 4160625527, 'write_volume': 70331457, 'read_io_pattern': 'uncl', 'write_io_pattern': 'seq', 'read_io_size': 822258.0092885375, 'write_io_size': 161.99470009835107, 'ioi_bw': 1188471.0629213485}]\n",
      "Found job_folder: 371911, is_folder:True\n",
      "job_folder: 371911\n",
      "    Browsing 371911...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371911.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371911.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371911.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371911.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371911.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371911.csv\n",
      "---shape of df : 723\n",
      "{'node_count': 1, 'events': [0, 1], 'read_volumes': [0, 4431310874], 'read_bw': [0, 6206317.75070028], 'write_volumes': [0, 74876180], 'write_bw': [0, 105015.68022440393], 'read_pattern': ['Uncl', 'Uncl'], 'write_pattern': ['Uncl', 'Seq'], 'read_operations': [0, 5477], 'write_operations': [0, 462220]}\n",
      "[{'job_id': '371911', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}, {'job_id': '371911', 'nodes': 1, 'read_volume': 4431310874, 'write_volume': 74876180, 'read_io_pattern': 'uncl', 'write_io_pattern': 'seq', 'read_io_size': 809076.296147526, 'write_io_size': 161.9925143870884, 'ioi_bw': 1262266.6861849367}]\n",
      "Found job_folder: 371906, is_folder:True\n",
      "job_folder: 371906\n",
      "    Browsing 371906...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371906.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371906.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371906.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371906.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371906.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371906.csv\n",
      "---shape of df : 17\n",
      "{'node_count': 1, 'events': [0, 1, 2], 'read_volumes': [832678, 36154, 0], 'read_bw': [832678.0, 36154.0, 0], 'write_volumes': [0, 9052112312, 0], 'write_bw': [0, 822919301.0909091, 0], 'read_pattern': ['Uncl', 'Uncl', 'Uncl'], 'write_pattern': ['Uncl', 'Str', 'Uncl'], 'read_operations': [9, 15, 0], 'write_operations': [0, 988, 0]}\n",
      "[{'job_id': '371906', 'nodes': 1, 'read_volume': 832678, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 92519.77777777778, 'write_io_size': 0, 'ioi_bw': 166535.6}, {'job_id': '371906', 'nodes': 1, 'read_volume': 36154, 'write_volume': 9052112312, 'read_io_pattern': 'uncl', 'write_io_pattern': 'str', 'read_io_size': 2410.266666666667, 'write_io_size': 9162056.995951418, 'ioi_bw': 164591091.01818183}, {'job_id': '371906', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}]\n",
      "Found job_folder: 371924, is_folder:True\n",
      "job_folder: 371924\n",
      "    Browsing 371924...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371924.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371924.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371924.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371924.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371924.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371924.csv\n",
      "---shape of df : 1\n",
      "Found job_folder: 371910, is_folder:True\n",
      "job_folder: 371910\n",
      "    Browsing 371910...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371910.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371910.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371910.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371910.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371910.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371910.csv\n",
      "---shape of df : 724\n",
      "{'node_count': 1, 'events': [0, 1], 'read_volumes': [0, 4190259001], 'read_bw': [0, 5868710.085434173], 'write_volumes': [0, 70825315], 'write_bw': [0, 99195.11904761905], 'read_pattern': ['Uncl', 'Uncl'], 'write_pattern': ['Uncl', 'Seq'], 'read_operations': [0, 5062], 'write_operations': [0, 437306]}\n",
      "[{'job_id': '371910', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}, {'job_id': '371910', 'nodes': 1, 'read_volume': 4190259001, 'write_volume': 70825315, 'read_io_pattern': 'uncl', 'write_io_pattern': 'seq', 'read_io_size': 827787.238443303, 'write_io_size': 161.95825120167572, 'ioi_bw': 1193581.0408963584}]\n",
      "Found job_folder: 371903, is_folder:True\n",
      "job_folder: 371903\n",
      "    Browsing 371903...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371903.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371903.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371903.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371903.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371903.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371903.csv\n",
      "---shape of df : 18\n",
      "{'node_count': 1, 'events': [0, 1, 2], 'read_volumes': [883728, 0, 0], 'read_bw': [441864.0, 0, 0], 'write_volumes': [0, 9011355062, 0], 'write_bw': [0, 819214096.5454545, 0], 'read_pattern': ['Uncl', 'Uncl', 'Uncl'], 'write_pattern': ['Uncl', 'Str', 'Uncl'], 'read_operations': [22, 0, 0], 'write_operations': [0, 1016, 0]}\n",
      "[{'job_id': '371903', 'nodes': 1, 'read_volume': 883728, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 40169.454545454544, 'write_io_size': 0, 'ioi_bw': 88372.8}, {'job_id': '371903', 'nodes': 1, 'read_volume': 0, 'write_volume': 9011355062, 'read_io_pattern': 'uncl', 'write_io_pattern': 'str', 'read_io_size': 0, 'write_io_size': 8869443.958661417, 'ioi_bw': 163842819.3090909}, {'job_id': '371903', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}]\n",
      "Found job_folder: 371902, is_folder:True\n",
      "job_folder: 371902\n",
      "    Browsing 371902...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371902.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371902.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371902.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371902.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371902.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371902.csv\n",
      "---shape of df : 18\n",
      "{'node_count': 1, 'events': [0, 1, 2], 'read_volumes': [868614, 0, 0], 'read_bw': [434307.0, 0, 0], 'write_volumes': [0, 9034495302, 0], 'write_bw': [0, 821317754.7272727, 0], 'read_pattern': ['Uncl', 'Uncl', 'Uncl'], 'write_pattern': ['Uncl', 'Str', 'Uncl'], 'read_operations': [22, 0, 0], 'write_operations': [0, 984, 0]}\n",
      "[{'job_id': '371902', 'nodes': 1, 'read_volume': 868614, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 39482.454545454544, 'write_io_size': 0, 'ioi_bw': 86861.4}, {'job_id': '371902', 'nodes': 1, 'read_volume': 0, 'write_volume': 9034495302, 'read_io_pattern': 'uncl', 'write_io_pattern': 'str', 'read_io_size': 0, 'write_io_size': 9181397.664634146, 'ioi_bw': 164263550.94545454}, {'job_id': '371902', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}]\n",
      "Found job_folder: 371900, is_folder:True\n",
      "job_folder: 371900\n",
      "    Browsing 371900...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371900.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371900.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371900.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371900.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371900.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371900.csv\n",
      "---shape of df : 1\n",
      "Found job_folder: 371902.csv, is_folder:False\n",
      "Skipping 371902.csv...\n",
      "Found job_folder: 371908, is_folder:True\n",
      "job_folder: 371908\n",
      "    Browsing 371908...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371908.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371908.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371908.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371908.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371908.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371908.csv\n",
      "---shape of df : 720\n",
      "{'node_count': 1, 'events': [0, 1], 'read_volumes': [0, 4300760675], 'read_bw': [0, 6048889.838255977], 'write_volumes': [0, 72590838], 'write_bw': [0, 102384.82087447109], 'read_pattern': ['Uncl', 'Uncl'], 'write_pattern': ['Uncl', 'Seq'], 'read_operations': [0, 5187], 'write_operations': [0, 448133]}\n",
      "[{'job_id': '371908', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}, {'job_id': '371908', 'nodes': 1, 'read_volume': 4300760675, 'write_volume': 72590838, 'read_io_pattern': 'uncl', 'write_io_pattern': 'seq', 'read_io_size': 829142.2161172162, 'write_io_size': 161.985031229568, 'ioi_bw': 1230254.9318260897}]\n",
      "Found job_folder: 371905, is_folder:True\n",
      "job_folder: 371905\n",
      "    Browsing 371905...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371905.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371905.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371905.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371905.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371905.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371905.csv\n",
      "---shape of df : 18\n",
      "{'node_count': 1, 'events': [0, 1, 2], 'read_volumes': [883728, 0, 0], 'read_bw': [441864.0, 0, 0], 'write_volumes': [0, 9032257893, 0], 'write_bw': [0, 821114353.9090909, 0], 'read_pattern': ['Uncl', 'Uncl', 'Uncl'], 'write_pattern': ['Uncl', 'Str', 'Uncl'], 'read_operations': [22, 0, 0], 'write_operations': [0, 1015, 0]}\n",
      "[{'job_id': '371905', 'nodes': 1, 'read_volume': 883728, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 40169.454545454544, 'write_io_size': 0, 'ioi_bw': 88372.8}, {'job_id': '371905', 'nodes': 1, 'read_volume': 0, 'write_volume': 9032257893, 'read_io_pattern': 'uncl', 'write_io_pattern': 'str', 'read_io_size': 0, 'write_io_size': 8898776.249261083, 'ioi_bw': 164222870.78181818}, {'job_id': '371905', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}]\n",
      "Found job_folder: 371907, is_folder:True\n",
      "job_folder: 371907\n",
      "    Browsing 371907...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371907.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371907.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371907.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371907.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371907.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371907.csv\n",
      "---shape of df : 722\n",
      "{'node_count': 1, 'events': [0, 1, 699], 'read_volumes': [0, 84574996, 0], 'read_bw': [0, 4974999.764705882, 0], 'write_volumes': [0, 14901919, 0], 'write_bw': [0, 931369.9375, 0], 'read_pattern': ['Uncl', 'Uncl', 'Uncl'], 'write_pattern': ['Uncl', 'Seq', 'Uncl'], 'read_operations': [0, 159, 0], 'write_operations': [0, 7364, 0]}\n",
      "[{'job_id': '371907', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}, {'job_id': '371907', 'nodes': 1, 'read_volume': 84574996, 'write_volume': 14901919, 'read_io_pattern': 'uncl', 'write_io_pattern': 'seq', 'read_io_size': 531918.213836478, 'write_io_size': 2023.6174633351438, 'ioi_bw': 1181273.9404411765}, {'job_id': '371907', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}]\n",
      "Found job_folder: 371904, is_folder:True\n",
      "job_folder: 371904\n",
      "    Browsing 371904...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371904.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371904.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371904.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371904.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371904.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371904.csv\n",
      "---shape of df : 18\n",
      "{'node_count': 1, 'events': [0, 1], 'read_volumes': [849622, 19210], 'read_bw': [424811.0, 19210.0], 'write_volumes': [0, 9053429170], 'write_bw': [0, 754452430.8333334], 'read_pattern': ['Uncl', 'Uncl'], 'write_pattern': ['Uncl', 'Str'], 'read_operations': [13, 11], 'write_operations': [0, 1022]}\n",
      "[{'job_id': '371904', 'nodes': 1, 'read_volume': 849622, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 65355.53846153846, 'write_io_size': 0, 'ioi_bw': 84962.2}, {'job_id': '371904', 'nodes': 1, 'read_volume': 19210, 'write_volume': 9053429170, 'read_io_pattern': 'uncl', 'write_io_pattern': 'str', 'read_io_size': 1746.3636363636363, 'write_io_size': 8858541.26223092, 'ioi_bw': 150894328.1666667}]\n",
      "Found job_folder: 371909, is_folder:True\n",
      "job_folder: 371909\n",
      "    Browsing 371909...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371909.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371909.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371909.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371909.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371909.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371909.csv\n",
      "---shape of df : 725\n",
      "{'node_count': 1, 'events': [0, 1], 'read_volumes': [0, 3978870378], 'read_bw': [0, 5557081.533519553], 'write_volumes': [0, 67218267], 'write_bw': [0, 94011.56223776223], 'read_pattern': ['Uncl', 'Uncl'], 'write_pattern': ['Uncl', 'Seq'], 'read_operations': [0, 4859], 'write_operations': [0, 414943]}\n",
      "[{'job_id': '371909', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}, {'job_id': '371909', 'nodes': 1, 'read_volume': 3978870378, 'write_volume': 67218267, 'read_io_pattern': 'uncl', 'write_io_pattern': 'seq', 'read_io_size': 818866.099608973, 'write_io_size': 161.99397748606435, 'ioi_bw': 1130218.619151463}]\n",
      "----\n",
      "Processing workflow : LQCD-64873bafcc9340246f412faf...\n",
      "Found job_folder: 367660, is_folder:True\n",
      "job_folder: 367660\n",
      "    Browsing 367660...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 367660.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 367660.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 367660.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 367660.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 367660.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 367660.csv\n",
      "---shape of df : 1\n",
      "Found job_folder: 367661, is_folder:True\n",
      "job_folder: 367661\n",
      "    Browsing 367661...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 367661.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 367661.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 367661.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 367661.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 367661.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 367661.csv\n",
      "---shape of df : 36\n",
      "{'node_count': 1, 'events': [0, 1, 8, 13, 17, 21, 26], 'read_volumes': [0, 1213885936, 0, 0, 0, 0, 0], 'read_bw': [0, 404628645.3333333, 0, 0, 0, 0, 0], 'write_volumes': [0, 0, 1207981456, 1207969704, 1207989664, 1207989657, 2416088849], 'write_bw': [0, 0, 603990728.0, 1207969704.0, 603994832.0, 603994828.5, 805362949.6666666], 'read_pattern': ['Uncl', 'Seq', 'Uncl', 'Uncl', 'Uncl', 'Uncl', 'Uncl'], 'write_pattern': ['Uncl', 'Uncl', 'Seq', 'Seq', 'Seq', 'Seq', 'Seq'], 'read_operations': [0, 5227, 0, 0, 0, 0, 0], 'write_operations': [0, 0, 139821, 139815, 139822, 139822, 279667]}\n",
      "[{'job_id': '367661', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}, {'job_id': '367661', 'nodes': 1, 'read_volume': 1213885936, 'write_volume': 0, 'read_io_pattern': 'seq', 'write_io_pattern': 'uncl', 'read_io_size': 232233.7738664626, 'write_io_size': 0, 'ioi_bw': 80925729.06666666}, {'job_id': '367661', 'nodes': 1, 'read_volume': 0, 'write_volume': 1207981456, 'read_io_pattern': 'uncl', 'write_io_pattern': 'seq', 'read_io_size': 0, 'write_io_size': 8639.485170324915, 'ioi_bw': 120798145.6}, {'job_id': '367661', 'nodes': 1, 'read_volume': 0, 'write_volume': 1207969704, 'read_io_pattern': 'uncl', 'write_io_pattern': 'seq', 'read_io_size': 0, 'write_io_size': 8639.771869971033, 'ioi_bw': 241593940.8}, {'job_id': '367661', 'nodes': 1, 'read_volume': 0, 'write_volume': 1207989664, 'read_io_pattern': 'uncl', 'write_io_pattern': 'seq', 'read_io_size': 0, 'write_io_size': 8639.482084364407, 'ioi_bw': 120798966.4}, {'job_id': '367661', 'nodes': 1, 'read_volume': 0, 'write_volume': 1207989657, 'read_io_pattern': 'uncl', 'write_io_pattern': 'seq', 'read_io_size': 0, 'write_io_size': 8639.482034300754, 'ioi_bw': 120798965.7}, {'job_id': '367661', 'nodes': 1, 'read_volume': 0, 'write_volume': 2416088849, 'read_io_pattern': 'uncl', 'write_io_pattern': 'seq', 'read_io_size': 0, 'write_io_size': 8639.163179781668, 'ioi_bw': 161072589.93333334}]\n",
      "Found job_folder: 367665, is_folder:True\n",
      "job_folder: 367665\n",
      "    Browsing 367665...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 367665.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 367665.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 367665.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 367665.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 367665.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 367665.csv\n",
      "---shape of df : 211\n",
      "{'node_count': 1, 'events': [0, 1, 25, 33, 56, 67, 74, 99, 124, 146, 149], 'read_volumes': [0, 1208019933, 0, 0, 0, 0, 10872294145, 9638211589, 9427222031, 9664099062, 0], 'read_bw': [0, 402673311.0, 0, 0, 0, 0, 1553184877.857143, 1927642317.8, 1885444406.2, 1932819812.4, 0], 'write_volumes': [0, 0, 4831874655, 4659780096, 4831862075, 4644419328, 0, 0, 0, 0, 0], 'write_bw': [0, 0, 483187465.5, 517753344.0, 483186207.5, 516046592.0, 0, 0, 0, 0, 0], 'read_pattern': ['Uncl', 'Seq', 'Uncl', 'Uncl', 'Uncl', 'Uncl', 'Seq', 'Seq', 'Seq', 'Seq', 'Uncl'], 'write_pattern': ['Uncl', 'Uncl', 'Seq', 'Seq', 'Seq', 'Seq', 'Uncl', 'Uncl', 'Uncl', 'Uncl', 'Uncl'], 'read_operations': [0, 4619, 0, 0, 0, 0, 41805, 37065, 36254, 37166, 0], 'write_operations': [0, 0, 18570, 17898, 18568, 17839, 0, 0, 0, 0, 0]}\n",
      "[{'job_id': '367665', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}, {'job_id': '367665', 'nodes': 1, 'read_volume': 1208019933, 'write_volume': 0, 'read_io_pattern': 'seq', 'write_io_pattern': 'uncl', 'read_io_size': 261532.78480190516, 'write_io_size': 0, 'ioi_bw': 80534662.2}, {'job_id': '367665', 'nodes': 1, 'read_volume': 0, 'write_volume': 4831874655, 'read_io_pattern': 'uncl', 'write_io_pattern': 'seq', 'read_io_size': 0, 'write_io_size': 260197.88126009694, 'ioi_bw': 96637493.1}, {'job_id': '367665', 'nodes': 1, 'read_volume': 0, 'write_volume': 4659780096, 'read_io_pattern': 'uncl', 'write_io_pattern': 'seq', 'read_io_size': 0, 'write_io_size': 260352.0, 'ioi_bw': 103550668.8}, {'job_id': '367665', 'nodes': 1, 'read_volume': 0, 'write_volume': 4831862075, 'read_io_pattern': 'uncl', 'write_io_pattern': 'seq', 'read_io_size': 0, 'write_io_size': 260225.23023481257, 'ioi_bw': 96637241.5}, {'job_id': '367665', 'nodes': 1, 'read_volume': 0, 'write_volume': 4644419328, 'read_io_pattern': 'uncl', 'write_io_pattern': 'seq', 'read_io_size': 0, 'write_io_size': 260352.0, 'ioi_bw': 103209318.4}, {'job_id': '367665', 'nodes': 1, 'read_volume': 10872294145, 'write_volume': 0, 'read_io_pattern': 'seq', 'write_io_pattern': 'uncl', 'read_io_size': 260071.62169596937, 'write_io_size': 0, 'ioi_bw': 310636975.5714286}, {'job_id': '367665', 'nodes': 1, 'read_volume': 9638211589, 'write_volume': 0, 'read_io_pattern': 'seq', 'write_io_pattern': 'uncl', 'read_io_size': 260035.38618642924, 'write_io_size': 0, 'ioi_bw': 385528463.56}, {'job_id': '367665', 'nodes': 1, 'read_volume': 9427222031, 'write_volume': 0, 'read_io_pattern': 'seq', 'write_io_pattern': 'uncl', 'read_io_size': 260032.6041540244, 'write_io_size': 0, 'ioi_bw': 377088881.24}, {'job_id': '367665', 'nodes': 1, 'read_volume': 9664099062, 'write_volume': 0, 'read_io_pattern': 'seq', 'write_io_pattern': 'uncl', 'read_io_size': 260025.26669536674, 'write_io_size': 0, 'ioi_bw': 386563962.48}, {'job_id': '367665', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}]\n",
      "Found job_folder: 367668, is_folder:True\n",
      "job_folder: 367668\n",
      "    Browsing 367668...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 367668.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 367668.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 367668.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 367668.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 367668.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 367668.csv\n",
      "---shape of df : 1\n",
      "----\n",
      "Processing workflow : deep_jobs.ipynb...\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to hold the data for each job\n",
    "data_for_jobs = {}\n",
    "\n",
    "# Loop through all items in the current directory\n",
    "for wf_folder in os.listdir():\n",
    "    # Check if the item is a folder and if its name is numeric (i.e., a job number)\n",
    "    print(f\"----\\nProcessing workflow : {wf_folder}...\")\n",
    "    if os.path.isdir(wf_folder):\n",
    "        # Initialize a dictionary to hold the data for this specific job\n",
    "        data_for_this_job = {}\n",
    "        # Loop through all JSON files in this folder\n",
    "        for job_folder in os.listdir(wf_folder):\n",
    "            is_folder = os.path.isdir(os.path.join(os.getcwd(), wf_folder, job_folder))\n",
    "            print(f\"Found job_folder: {job_folder}, is_folder:{is_folder}\")\n",
    "            # Ensure that we are dealing with job folder\n",
    "            if is_folder:\n",
    "                print(f\"job_folder: {job_folder}\")\n",
    "                # Initialize a DataFrame to hold the data for this specific job\n",
    "                df_for_this_job = pd.DataFrame()\n",
    "                print(f\"    Browsing {job_folder}...\")\n",
    "                for json_file in os.listdir(os.path.join(wf_folder, job_folder)):\n",
    "                    if is_file_extension(json_file, \"json\"):\n",
    "                        print(f\"      Processing {json_file}...\")\n",
    "                        # Construct the full path to the JSON file\n",
    "                        json_file_path = os.path.join(os.getcwd(), wf_folder, job_folder, json_file)\n",
    "                        \n",
    "                        # Read the JSON file into a list of lists\n",
    "                        with open(json_file_path, 'r') as f:\n",
    "                            json_data = json.load(f)\n",
    "                        \n",
    "                        # Create a temporary DataFrame from the JSON data\n",
    "                        df_temp = pd.DataFrame(json_data, columns=\n",
    "                                            get_column_names(json_file))\n",
    "                        \n",
    "                        # Merge the temporary DataFrame into the DataFrame for this job, based on the 'timestamp' column\n",
    "                        if df_for_this_job.empty:\n",
    "                            df_for_this_job = df_temp\n",
    "                        else:\n",
    "                            df_for_this_job = pd.merge(df_for_this_job, \n",
    "                                                       df_temp, \n",
    "                                                       on='timestamp', \n",
    "                                                       how='outer')\n",
    "                            \n",
    "                    # Save the DataFrame for this job to a CSV file\n",
    "                    csv_file_path = os.path.join(os.getcwd(), wf_folder, f\"{job_folder}.csv\")\n",
    "                    \n",
    "                    #df_for_this_job.to_csv(csv_file_path, index=False)\n",
    "                    print(f\"            Saving here... {job_folder}.csv\")\n",
    "                    \n",
    "                    # Add this DataFrame to the dictionary\n",
    "                    data_for_jobs[job_folder] = df_for_this_job\n",
    "                    \n",
    "                # Gather timeseries data for this job\n",
    "                print(f\"---shape of df : {df_for_this_job.shape[0]}\")\n",
    "                if df_for_this_job.shape[0] > 1:\n",
    "                    representation, phase_features = decompose_ioi_job(wf_folder, job_folder)\n",
    "                    print(representation)\n",
    "                    print(phase_features)\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping {job_folder}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'job_id': '371902', 'nodes': 1, 'read_volume': 868614, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 39482.454545454544, 'write_io_size': 0, 'ioi_bw': 86861.4}, {'job_id': '371902', 'nodes': 1, 'read_volume': 0, 'write_volume': 9034495302, 'read_io_pattern': 'uncl', 'write_io_pattern': 'str', 'read_io_size': 0, 'write_io_size': 9181397.664634146, 'ioi_bw': 164263550.94545454}, {'job_id': '371902', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}]\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "wf_folder = \"/home_nfs/mimounis/iosea-wp3-recommandation-system/dataset_generation/dataset_deep/ECMWF-649c3c40cc9340246f87cb58\"\n",
    "job_id = \"371902\"\n",
    "job_timeseries = get_job_timeseries_from_json_file(wf_folder, job_id)\n",
    "\n",
    "#%%capture\n",
    "logger.remove()\n",
    "def decompose_ioi_job(wf_folder, job_id):\n",
    "    with patch.object(ComplexDecomposer, 'get_job_timeseries') as mock_get_timeseries:\n",
    "        with patch.object(Configuration, 'get_kc_token') as mock_get_kc_token:\n",
    "            with patch.object(ComplexDecomposer, 'get_job_node_count') as mock_get_node_count:\n",
    "                mock_get_timeseries.return_value = get_job_timeseries_from_json_file(wf_folder, job_id)\n",
    "                mock_get_kc_token.return_value = 'token'\n",
    "                mock_get_node_count.return_value = 1\n",
    "                # init the job decomposer\n",
    "                #cd = ComplexDecomposer(v0_threshold=0.02)\n",
    "                cd = ComplexDecomposer(v0_threshold=0.01)\n",
    "                representation = cd.get_job_representation(merge_clusters=True)\n",
    "                phase_features = cd.get_phases_features(representation, job_id=job_id)\n",
    "                return representation, phase_features\n",
    "# Launch decomposition on the signal\n",
    "representation, phase_features = decompose_ioi_job(wf_folder, job_id)\n",
    "# Showing representation\n",
    "compute, reads, read_bw, writes, write_bw = representation[\"events\"], representation[\"read_volumes\"], representation[\"read_bw\"], representation[\"write_volumes\"], representation[\"write_bw\"]\n",
    "\n",
    "print(phase_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'job_id': '371902', 'nodes': 1, 'read_volume': 868614, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 39482.454545454544, 'write_io_size': 0, 'ioi_bw': 86861.4}, {'job_id': '371902', 'nodes': 1, 'read_volume': 0, 'write_volume': 9034495302, 'read_io_pattern': 'uncl', 'write_io_pattern': 'str', 'read_io_size': 0, 'write_io_size': 9181397.664634146, 'ioi_bw': 164263550.94545454}, {'job_id': '371902', 'nodes': 1, 'read_volume': 0, 'write_volume': 0, 'read_io_pattern': 'uncl', 'write_io_pattern': 'uncl', 'read_io_size': 0, 'write_io_size': 0, 'ioi_bw': 0.0}]\n"
     ]
    }
   ],
   "source": [
    "# Normalize signals to seconds and MB\n",
    "timestamps = (cd.timestamps.flatten() - cd.timestamps.flatten()[0])/5\n",
    "original_read =  cd.read_signal.flatten()/1e6\n",
    "original_write = cd.write_signal.flatten()/1e6\n",
    "\n",
    "read_bw_scaled = list(map(lambda x: x/1e6, read_bw))\n",
    "write_bw_scaled = list(map(lambda x: x/1e6, write_bw))\n",
    "\n",
    "env = simpy.Environment()\n",
    "nvram_bandwidth = {'read':  {'seq': 780, 'rand': 760},\n",
    "                    'write': {'seq': 515, 'rand': 505}}\n",
    "ssd_bandwidth = {'read':  {'seq': 1, 'rand': 1},\n",
    "                    'write': {'seq': 1, 'rand': 1}}\n",
    "\n",
    "ssd_tier = Tier(env, 'SSD', max_bandwidth=ssd_bandwidth, capacity=200e9)\n",
    "nvram_tier = Tier(env, 'NVRAM', max_bandwidth=nvram_bandwidth, capacity=80e9)\n",
    "data = simpy.Store(env)\n",
    "cluster = Cluster(env,  compute_nodes=1, cores_per_node=2,\n",
    "                    tiers=[ssd_tier, nvram_tier])\n",
    "app = Application(env, name=f\"job#{job_id}\",\n",
    "                    compute=compute,\n",
    "                    read=reads,\n",
    "                    write=writes,\n",
    "                    data=data,\n",
    "                    read_bw=read_bw_scaled,\n",
    "                    write_bw=write_bw_scaled)\n",
    "env.process(app.run(cluster, placement=[0]*(10*len(compute))))\n",
    "env.run()\n",
    "\n",
    "output = get_execution_signal_3(data, nbr_points=len(timestamps))\n",
    "sim_time = np.array(output[app.name][\"time\"])\n",
    "sim_read_bw = np.array(output[app.name][\"read_bw\"])\n",
    "sim_write_bw = np.array(output[app.name][\"write_bw\"])\n",
    "print(len(sim_time))\n",
    "print(len(timestamps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.plotly.v1+json": {
       "config": {
        "plotlyServerURL": "https://plot.ly"
       },
       "data": [
        {
         "line": {
          "width": 1.5
         },
         "mode": "lines+markers",
         "name": "original write signal from IOI",
         "text": [
          "class=0",
          "class=0",
          "class=0",
          "class=0",
          "class=0",
          "class=0",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=0"
         ],
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17
         ],
         "y": [
          0,
          0,
          0,
          0,
          0,
          0.131072,
          872.448408,
          707.461608,
          791.280104,
          770.243048,
          759.822824,
          759.757288,
          837.769738,
          837.704202,
          905.230892,
          905.230892,
          887.546298,
          18.8028
         ]
        },
        {
         "line": {
          "dash": "dash",
          "width": 1.5
         },
         "mode": "lines+markers",
         "name": "reconstructed write signal from IOI",
         "text": [
          "class=0",
          "class=0",
          "class=0",
          "class=0",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=1",
          "class=0",
          "class=0"
         ],
         "type": "scatter",
         "x": [
          0,
          1,
          2,
          3,
          4,
          5,
          6,
          7,
          8,
          9,
          10,
          11,
          12,
          13,
          14,
          15,
          16,
          17
         ],
         "y": [
          0,
          0,
          0,
          0,
          821.3177547272727,
          821.3177547272727,
          821.3177547272727,
          821.3177547272727,
          821.3177547272727,
          821.3177547272727,
          821.3177547272727,
          821.3177547272727,
          821.3177547272727,
          821.3177547272727,
          821.3177547272727,
          821.3177547272727,
          0,
          0
         ]
        }
       ],
       "layout": {
        "height": 500,
        "legend": {
         "orientation": "h",
         "title": {
          "text": "Signals"
         },
         "yanchor": "top"
        },
        "template": {
         "data": {
          "bar": [
           {
            "error_x": {
             "color": "#2a3f5f"
            },
            "error_y": {
             "color": "#2a3f5f"
            },
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "bar"
           }
          ],
          "barpolar": [
           {
            "marker": {
             "line": {
              "color": "#E5ECF6",
              "width": 0.5
             },
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "barpolar"
           }
          ],
          "carpet": [
           {
            "aaxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "baxis": {
             "endlinecolor": "#2a3f5f",
             "gridcolor": "white",
             "linecolor": "white",
             "minorgridcolor": "white",
             "startlinecolor": "#2a3f5f"
            },
            "type": "carpet"
           }
          ],
          "choropleth": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "choropleth"
           }
          ],
          "contour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "contour"
           }
          ],
          "contourcarpet": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "contourcarpet"
           }
          ],
          "heatmap": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmap"
           }
          ],
          "heatmapgl": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "heatmapgl"
           }
          ],
          "histogram": [
           {
            "marker": {
             "pattern": {
              "fillmode": "overlay",
              "size": 10,
              "solidity": 0.2
             }
            },
            "type": "histogram"
           }
          ],
          "histogram2d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2d"
           }
          ],
          "histogram2dcontour": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "histogram2dcontour"
           }
          ],
          "mesh3d": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "type": "mesh3d"
           }
          ],
          "parcoords": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "parcoords"
           }
          ],
          "pie": [
           {
            "automargin": true,
            "type": "pie"
           }
          ],
          "scatter": [
           {
            "fillpattern": {
             "fillmode": "overlay",
             "size": 10,
             "solidity": 0.2
            },
            "type": "scatter"
           }
          ],
          "scatter3d": [
           {
            "line": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatter3d"
           }
          ],
          "scattercarpet": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattercarpet"
           }
          ],
          "scattergeo": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergeo"
           }
          ],
          "scattergl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattergl"
           }
          ],
          "scattermapbox": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scattermapbox"
           }
          ],
          "scatterpolar": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolar"
           }
          ],
          "scatterpolargl": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterpolargl"
           }
          ],
          "scatterternary": [
           {
            "marker": {
             "colorbar": {
              "outlinewidth": 0,
              "ticks": ""
             }
            },
            "type": "scatterternary"
           }
          ],
          "surface": [
           {
            "colorbar": {
             "outlinewidth": 0,
             "ticks": ""
            },
            "colorscale": [
             [
              0,
              "#0d0887"
             ],
             [
              0.1111111111111111,
              "#46039f"
             ],
             [
              0.2222222222222222,
              "#7201a8"
             ],
             [
              0.3333333333333333,
              "#9c179e"
             ],
             [
              0.4444444444444444,
              "#bd3786"
             ],
             [
              0.5555555555555556,
              "#d8576b"
             ],
             [
              0.6666666666666666,
              "#ed7953"
             ],
             [
              0.7777777777777778,
              "#fb9f3a"
             ],
             [
              0.8888888888888888,
              "#fdca26"
             ],
             [
              1,
              "#f0f921"
             ]
            ],
            "type": "surface"
           }
          ],
          "table": [
           {
            "cells": {
             "fill": {
              "color": "#EBF0F8"
             },
             "line": {
              "color": "white"
             }
            },
            "header": {
             "fill": {
              "color": "#C8D4E3"
             },
             "line": {
              "color": "white"
             }
            },
            "type": "table"
           }
          ]
         },
         "layout": {
          "annotationdefaults": {
           "arrowcolor": "#2a3f5f",
           "arrowhead": 0,
           "arrowwidth": 1
          },
          "autotypenumbers": "strict",
          "coloraxis": {
           "colorbar": {
            "outlinewidth": 0,
            "ticks": ""
           }
          },
          "colorscale": {
           "diverging": [
            [
             0,
             "#8e0152"
            ],
            [
             0.1,
             "#c51b7d"
            ],
            [
             0.2,
             "#de77ae"
            ],
            [
             0.3,
             "#f1b6da"
            ],
            [
             0.4,
             "#fde0ef"
            ],
            [
             0.5,
             "#f7f7f7"
            ],
            [
             0.6,
             "#e6f5d0"
            ],
            [
             0.7,
             "#b8e186"
            ],
            [
             0.8,
             "#7fbc41"
            ],
            [
             0.9,
             "#4d9221"
            ],
            [
             1,
             "#276419"
            ]
           ],
           "sequential": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ],
           "sequentialminus": [
            [
             0,
             "#0d0887"
            ],
            [
             0.1111111111111111,
             "#46039f"
            ],
            [
             0.2222222222222222,
             "#7201a8"
            ],
            [
             0.3333333333333333,
             "#9c179e"
            ],
            [
             0.4444444444444444,
             "#bd3786"
            ],
            [
             0.5555555555555556,
             "#d8576b"
            ],
            [
             0.6666666666666666,
             "#ed7953"
            ],
            [
             0.7777777777777778,
             "#fb9f3a"
            ],
            [
             0.8888888888888888,
             "#fdca26"
            ],
            [
             1,
             "#f0f921"
            ]
           ]
          },
          "colorway": [
           "#636efa",
           "#EF553B",
           "#00cc96",
           "#ab63fa",
           "#FFA15A",
           "#19d3f3",
           "#FF6692",
           "#B6E880",
           "#FF97FF",
           "#FECB52"
          ],
          "font": {
           "color": "#2a3f5f"
          },
          "geo": {
           "bgcolor": "white",
           "lakecolor": "white",
           "landcolor": "#E5ECF6",
           "showlakes": true,
           "showland": true,
           "subunitcolor": "white"
          },
          "hoverlabel": {
           "align": "left"
          },
          "hovermode": "closest",
          "mapbox": {
           "style": "light"
          },
          "paper_bgcolor": "white",
          "plot_bgcolor": "#E5ECF6",
          "polar": {
           "angularaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "radialaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "scene": {
           "xaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "yaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           },
           "zaxis": {
            "backgroundcolor": "#E5ECF6",
            "gridcolor": "white",
            "gridwidth": 2,
            "linecolor": "white",
            "showbackground": true,
            "ticks": "",
            "zerolinecolor": "white"
           }
          },
          "shapedefaults": {
           "line": {
            "color": "#2a3f5f"
           }
          },
          "ternary": {
           "aaxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "baxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           },
           "bgcolor": "#E5ECF6",
           "caxis": {
            "gridcolor": "white",
            "linecolor": "white",
            "ticks": ""
           }
          },
          "title": {
           "x": 0.05
          },
          "xaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          },
          "yaxis": {
           "automargin": true,
           "gridcolor": "white",
           "linecolor": "white",
           "ticks": "",
           "title": {
            "standoff": 15
           },
           "zerolinecolor": "white",
           "zerolinewidth": 2
          }
         }
        },
        "title": {
         "text": "IOI Signal - comparison between real and reconstructed"
        },
        "width": 800,
        "yaxis": {
         "title": {
          "text": "volume conveyed by the application"
         }
        }
       }
      },
      "text/html": [
       "<div>                            <div id=\"09f3ca7b-a4e0-4b5e-8e21-c944e4814cbc\" class=\"plotly-graph-div\" style=\"height:500px; width:800px;\"></div>            <script type=\"text/javascript\">                require([\"plotly\"], function(Plotly) {                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"09f3ca7b-a4e0-4b5e-8e21-c944e4814cbc\")) {                    Plotly.newPlot(                        \"09f3ca7b-a4e0-4b5e-8e21-c944e4814cbc\",                        [{\"line\":{\"width\":1.5},\"mode\":\"lines+markers\",\"name\":\"original write signal from IOI\",\"text\":[\"class=0\",\"class=0\",\"class=0\",\"class=0\",\"class=0\",\"class=0\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=0\"],\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\"y\":[0.0,0.0,0.0,0.0,0.0,0.131072,872.448408,707.461608,791.280104,770.243048,759.822824,759.757288,837.769738,837.704202,905.230892,905.230892,887.546298,18.8028],\"type\":\"scatter\"},{\"line\":{\"dash\":\"dash\",\"width\":1.5},\"mode\":\"lines+markers\",\"name\":\"reconstructed write signal from IOI\",\"text\":[\"class=0\",\"class=0\",\"class=0\",\"class=0\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=1\",\"class=0\",\"class=0\"],\"x\":[0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17],\"y\":[0.0,0.0,0.0,0.0,821.3177547272727,821.3177547272727,821.3177547272727,821.3177547272727,821.3177547272727,821.3177547272727,821.3177547272727,821.3177547272727,821.3177547272727,821.3177547272727,821.3177547272727,821.3177547272727,0.0,0.0],\"type\":\"scatter\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"legend\":{\"orientation\":\"h\",\"yanchor\":\"top\",\"title\":{\"text\":\"Signals\"}},\"width\":800,\"height\":500,\"title\":{\"text\":\"IOI Signal - comparison between real and reconstructed\"},\"yaxis\":{\"title\":{\"text\":\"volume conveyed by the application\"}}},                        {\"responsive\": true}                    ).then(function(){\n",
       "                            \n",
       "var gd = document.getElementById('09f3ca7b-a4e0-4b5e-8e21-c944e4814cbc');\n",
       "var x = new MutationObserver(function (mutations, observer) {{\n",
       "        var display = window.getComputedStyle(gd).display;\n",
       "        if (!display || display === 'none') {{\n",
       "            console.log([gd, 'removed!']);\n",
       "            Plotly.purge(gd);\n",
       "            observer.disconnect();\n",
       "        }}\n",
       "}});\n",
       "\n",
       "// Listen for the removal of the full notebook cells\n",
       "var notebookContainer = gd.closest('#notebook-container');\n",
       "if (notebookContainer) {{\n",
       "    x.observe(notebookContainer, {childList: true});\n",
       "}}\n",
       "\n",
       "// Listen for the clearing of the current output cell\n",
       "var outputEl = gd.closest('.output');\n",
       "if (outputEl) {{\n",
       "    x.observe(outputEl, {childList: true});\n",
       "}}\n",
       "\n",
       "                        })                };                });            </script>        </div>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_detected_phases_compare(job_id, merge=True, show_phases=False, \n",
    "                             ts=(timestamps, original_write, sim_write_bw), \n",
    "                             width=800, height=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Define the path to the dataset folder\n",
    "dataset_folder = './dataset_generation/dataset_deep/'\n",
    "\n",
    "# Initialize dictionary to hold job data\n",
    "workflow_data = defaultdict(list)\n",
    "\n",
    "# Step 1: Load Data\n",
    "for root, dirs, files in os.walk(dataset_folder):\n",
    "    for filename in files:\n",
    "        if filename == 'volume.json':\n",
    "            filepath = os.path.join(root, filename)\n",
    "            print(filepath)\n",
    "            with open(filepath, 'r') as f:\n",
    "                volume_data = json.load(f)\n",
    "            \n",
    "            # Extract the job start and end timestamps\n",
    "            job_id = root.split('/')[-2]  # Assuming job ID is the parent folder name\n",
    "            start_time = volume_data[0][0]\n",
    "            end_time = volume_data[-1][0]\n",
    "            \n",
    "            # Store data in the dictionary\n",
    "            workflow_data[job_id] = {'start_time': start_time, 'end_time': end_time}\n",
    "\n",
    "# Step 2: Sort Jobs by Timestamp\n",
    "sorted_jobs = sorted(workflow_data.items(), key=lambda x: x[1]['start_time'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'371912': {'start_time': 1687960785000, 'end_time': 1687964390000}, '371913': {'start_time': 1687960790000, 'end_time': 1687964390000}, '371911': {'start_time': 1687960780000, 'end_time': 1687964390000}, '371906': {'start_time': 1687960695000, 'end_time': 1687960775000}, '371924': {'start_time': 1687964500000, 'end_time': 1687964500000}, '371910': {'start_time': 1687960775000, 'end_time': 1687964390000}, '371903': {'start_time': 1687960690000, 'end_time': 1687960775000}, '371902': {'start_time': 1687960690000, 'end_time': 1687960775000}, '371900': {'start_time': 1687960640000, 'end_time': 1687960640000}, '371908': {'start_time': 1687960765000, 'end_time': 1687964360000}, '371905': {'start_time': 1687960690000, 'end_time': 1687960775000}, '371907': {'start_time': 1687960755000, 'end_time': 1687964360000}, '371904': {'start_time': 1687960690000, 'end_time': 1687960775000}, '371909': {'start_time': 1687960770000, 'end_time': 1687964390000}})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "# Function to check file extension\n",
    "def is_file_extension(filename, extension):\n",
    "    return filename.endswith(f'.{extension}')\n",
    "\n",
    "# Initialize dictionary to hold job data\n",
    "workflow_data = defaultdict(list)\n",
    "\n",
    "# Default workflow folder\n",
    "wf_folder = \"/home_nfs/mimounis/iosea-wp3-recommandation-system/dataset_generation/dataset_deep/ECMWF-649c3c40cc9340246f87cb58\"\n",
    "\n",
    "# Loop through all job folders in the workflow folder\n",
    "for job_folder in os.listdir(wf_folder):\n",
    "    #print(f\"----\\nProcessing job : {job_folder}...\")\n",
    "    \n",
    "    job_folder_path = os.path.join(wf_folder, job_folder)\n",
    "    \n",
    "    # Ensure that we are dealing with job folder\n",
    "    if os.path.isdir(job_folder_path):\n",
    "        json_file_path = os.path.join(job_folder_path, 'volume.json')\n",
    "        \n",
    "        # Check if volume.json exists in the job folder\n",
    "        if os.path.exists(json_file_path):\n",
    "            #print(f\"      Processing volume.json for job {job_folder}...\")\n",
    "            \n",
    "            # Read the JSON file into a list of lists\n",
    "            with open(json_file_path, 'r') as f:\n",
    "                volume_data = json.load(f)\n",
    "            \n",
    "            # Extract the job start and end timestamps\n",
    "            start_time = volume_data[0][0]\n",
    "            end_time = volume_data[-1][0]\n",
    "            \n",
    "            # Store data in the dictionary\n",
    "            workflow_data[job_folder] = {'start_time': start_time, 'end_time': end_time}\n",
    "\n",
    "# Calculate Delays and Check for Overlaps\n",
    "previous_end_time = 0\n",
    "for i, (job_id, job_data) in enumerate(sorted_jobs):\n",
    "    delay = job_data['start_time'] - previous_end_time\n",
    "    print(f\"Delay between Job {sorted_jobs[i-1][0]} and Job {job_id}: {delay} ms\")\n",
    "    \n",
    "    if i > 0 and job_data['start_time'] < sorted_jobs[i-1][1]['end_time']:\n",
    "        print(f\"Job {job_id} and Job {sorted_jobs[i-1][0]} have overlapping time spans.\")\n",
    "    \n",
    "    previous_end_time = job_data['end_time']\n",
    "\n",
    "    if delay < 5000:  # 5 seconds\n",
    "        print(f\"Guessing 'afterok' dependency between Job {sorted_jobs[i-1][0]} and Job {job_id}\")\n",
    "\n",
    "print(workflow_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['371903', '371911', 5000], ['371902', '371911', 5000], ['371905', '371911', 5000], ['371904', '371911', 5000], ['371906', '371911', 5000]]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Example sorted_jobs\n",
    "sorted_jobs = sorted(\n",
    "    {\n",
    "        '371912': {'start_time': 1687960785000, 'end_time': 1687964390000},\n",
    "        '371913': {'start_time': 1687960790000, 'end_time': 1687964390000},\n",
    "        '371911': {'start_time': 1687960780000, 'end_time': 1687964390000},\n",
    "        '371906': {'start_time': 1687960695000, 'end_time': 1687960775000},\n",
    "        '371924': {'start_time': 1687964500000, 'end_time': 1687964500000},\n",
    "        '371910': {'start_time': 1687960775000, 'end_time': 1687964390000},\n",
    "        '371903': {'start_time': 1687960690000, 'end_time': 1687960775000},\n",
    "        '371902': {'start_time': 1687960690000, 'end_time': 1687960775000},\n",
    "        '371900': {'start_time': 1687960640000, 'end_time': 1687960640000},\n",
    "        '371908': {'start_time': 1687960765000, 'end_time': 1687964360000},\n",
    "        '371905': {'start_time': 1687960690000, 'end_time': 1687960775000},\n",
    "        '371907': {'start_time': 1687960755000, 'end_time': 1687964360000},\n",
    "        '371904': {'start_time': 1687960690000, 'end_time': 1687960775000},\n",
    "        '371909': {'start_time': 1687960770000, 'end_time': 1687964390000}\n",
    "    }.items(),\n",
    "    key=lambda x: x[1]['start_time']\n",
    ")\n",
    "\n",
    "# Initialize dependency dictionaries\n",
    "dependencies = {\n",
    "    'sequential': [],\n",
    "    'parallel': [],\n",
    "    'after': [],\n",
    "    'delay': []\n",
    "}\n",
    "\n",
    "# Threshold for 'after' dependency in milliseconds\n",
    "threshold = 1000  # 1 second\n",
    "# sorted_jobs = workflow_data\n",
    "# Loop through sorted jobs\n",
    "for i in range(len(sorted_jobs)):\n",
    "    current_job_id, current_job_data = sorted_jobs[i]\n",
    "    current_start, current_end = current_job_data['start_time'], current_job_data['end_time']\n",
    "    \n",
    "    for j in range(i+1, len(sorted_jobs)):\n",
    "        next_job_id, next_job_data = sorted_jobs[j]\n",
    "        next_start, next_end = next_job_data['start_time'], next_job_data['end_time']\n",
    "        \n",
    "        # Check for Sequential dependency\n",
    "        if current_end <= next_start:\n",
    "            dependencies['sequential'].append([current_job_id, next_job_id])\n",
    "        \n",
    "        # Check for Parallel dependency\n",
    "        if current_start < next_end and current_end > next_start:\n",
    "            dependencies['parallel'].append([current_job_id, next_job_id])\n",
    "        \n",
    "        # Check for 'After' dependency\n",
    "        if next_start - current_end <= threshold:\n",
    "            dependencies['after'].append([current_job_id, next_job_id])\n",
    "        \n",
    "        # Check for 'Delay' dependency (here, we specify an example delay of 5 seconds)\n",
    "        if 5000 <= next_start - current_end <= 5000 + threshold:\n",
    "            dependencies['delay'].append([current_job_id, next_job_id, 5000])\n",
    "\n",
    "print(dependencies[\"delay\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('371900', {'start_time': 1687960640000, 'end_time': 1687960640000}), ('371903', {'start_time': 1687960690000, 'end_time': 1687960775000}), ('371902', {'start_time': 1687960690000, 'end_time': 1687960775000}), ('371905', {'start_time': 1687960690000, 'end_time': 1687960775000}), ('371904', {'start_time': 1687960690000, 'end_time': 1687960775000}), ('371906', {'start_time': 1687960695000, 'end_time': 1687960775000}), ('371907', {'start_time': 1687960755000, 'end_time': 1687964360000}), ('371908', {'start_time': 1687960765000, 'end_time': 1687964360000}), ('371909', {'start_time': 1687960770000, 'end_time': 1687964390000}), ('371910', {'start_time': 1687960775000, 'end_time': 1687964390000}), ('371911', {'start_time': 1687960780000, 'end_time': 1687964390000}), ('371912', {'start_time': 1687960785000, 'end_time': 1687964390000}), ('371913', {'start_time': 1687960790000, 'end_time': 1687964390000}), ('371924', {'start_time': 1687964500000, 'end_time': 1687964500000})]\n"
     ]
    }
   ],
   "source": [
    "print((sorted_jobs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            start_time  end_time\n",
      "start_time    1.000000  0.299907\n",
      "end_time      0.299907  1.000000\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {    \n",
    "    '371912': {'start_time': 1687960785000, 'end_time': 1687964390000},\n",
    "    '371913': {'start_time': 1687960790000, 'end_time': 1687964390000},\n",
    "    '371911': {'start_time': 1687960780000, 'end_time': 1687964390000},\n",
    "    '371906': {'start_time': 1687960695000, 'end_time': 1687960775000},\n",
    "    '371924': {'start_time': 1687964500000, 'end_time': 1687964500000},\n",
    "    '371910': {'start_time': 1687960775000, 'end_time': 1687964390000},\n",
    "    '371903': {'start_time': 1687960690000, 'end_time': 1687960775000},\n",
    "    '371902': {'start_time': 1687960690000, 'end_time': 1687960775000},\n",
    "    '371900': {'start_time': 1687960640000, 'end_time': 1687960640000},\n",
    "    '371908': {'start_time': 1687960765000, 'end_time': 1687964360000},\n",
    "    '371905': {'start_time': 1687960690000, 'end_time': 1687960775000},\n",
    "    '371907': {'start_time': 1687960755000, 'end_time': 1687964360000},\n",
    "    '371904': {'start_time': 1687960690000, 'end_time': 1687960775000},\n",
    "    '371909': {'start_time': 1687960770000, 'end_time': 1687964390000}\n",
    "}\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame.from_dict(data, orient='index')\n",
    "\n",
    "# Compute the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "print(correlation_matrix)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iosea-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
