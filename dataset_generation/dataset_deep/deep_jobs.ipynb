{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        timestamp  bytesRead  bytesWritten\n",
      "0   1687960785000       10.0           5.0\n",
      "1   1687960790000       20.0          15.0\n",
      "2   1687960795000       30.0          25.0\n",
      "3   1687960800000        0.0           0.0\n",
      "4   1687960805000        0.0           0.0\n",
      "..            ...        ...           ...\n",
      "70  1687961135000        0.0           0.0\n",
      "71  1687961140000        0.0           0.0\n",
      "72  1687961145000       40.0          35.0\n",
      "73  1687961150000       50.0          45.0\n",
      "74  1687961155000       60.0          55.0\n",
      "\n",
      "[75 rows x 3 columns]\n",
      "{'timestamp': [1687960785000, 1687960790000, 1687960795000, 1687960800000, 1687960805000, 1687960810000, 1687960815000, 1687960820000, 1687960825000, 1687960830000, 1687960835000, 1687960840000, 1687960845000, 1687960850000, 1687960855000, 1687960860000, 1687960865000, 1687960870000, 1687960875000, 1687960880000, 1687960885000, 1687960890000, 1687960895000, 1687960900000, 1687960905000, 1687960910000, 1687960915000, 1687960920000, 1687960925000, 1687960930000, 1687960935000, 1687960940000, 1687960945000, 1687960950000, 1687960955000, 1687960960000, 1687960965000, 1687960970000, 1687960975000, 1687960980000, 1687960985000, 1687960990000, 1687960995000, 1687961000000, 1687961005000, 1687961010000, 1687961015000, 1687961020000, 1687961025000, 1687961030000, 1687961035000, 1687961040000, 1687961045000, 1687961050000, 1687961055000, 1687961060000, 1687961065000, 1687961070000, 1687961075000, 1687961080000, 1687961085000, 1687961090000, 1687961095000, 1687961100000, 1687961105000, 1687961110000, 1687961115000, 1687961120000, 1687961125000, 1687961130000, 1687961135000, 1687961140000, 1687961145000, 1687961150000, 1687961155000], 'bytesRead': [10.0, 20.0, 30.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 40.0, 50.0, 60.0], 'bytesWritten': [5.0, 15.0, 25.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 35.0, 45.0, 55.0]}\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class WorkflowSynthesizer:\n",
    "    \"\"\"\n",
    "    Class for synthesizing a workflow from a dictionary of jobs. \n",
    "    The jobs and the workflow are represented as dataframes with time-series data.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Initialize a new instance of the WorkflowSynthesizer class.\n",
    "        \"\"\"\n",
    "        self.workflow = pd.DataFrame()\n",
    "\n",
    "    def synthetize(self, data_for_jobs):\n",
    "        \"\"\"\n",
    "        Synthesize a workflow from the provided jobs.\n",
    "\n",
    "        Args:\n",
    "            data_for_jobs (dict): A dictionary where keys are job names \n",
    "            and values are DataFrames with columns 'timestamp', 'bytesRead', \n",
    "            and 'bytesWritten'.\n",
    "        \"\"\"\n",
    "        min_timestamp = min(df['timestamp'].min() for df in data_for_jobs.values())\n",
    "        max_timestamp = max(df['timestamp'].max() for df in data_for_jobs.values())\n",
    "        \n",
    "        # Create a DataFrame with a uniform timestamp range\n",
    "        synthetic_timestamps = np.arange(min_timestamp, max_timestamp + 1, 5000)\n",
    "        synthetic_df = pd.DataFrame({'timestamp': synthetic_timestamps})\n",
    "        \n",
    "        for job_name, job_df in data_for_jobs.items():\n",
    "            # Merge with the synthetic DataFrame to get uniform timestamps\n",
    "            merged_df = pd.merge(synthetic_df, job_df, on='timestamp', how='left').fillna(0)\n",
    "            \n",
    "            # Sum up the bytesRead and bytesWritten across all jobs\n",
    "            if self.workflow.empty:\n",
    "                self.workflow = merged_df\n",
    "            else:\n",
    "                self.workflow['bytesRead'] += merged_df['bytesRead']\n",
    "                self.workflow['bytesWritten'] += merged_df['bytesWritten']\n",
    "\n",
    "    def to_dict(self):\n",
    "        \"\"\"\n",
    "        Convert the synthesized workflow to a dictionary format.\n",
    "\n",
    "        Returns:\n",
    "            dict: The synthesized workflow in dictionary format.\n",
    "        \"\"\"\n",
    "        output = {}\n",
    "        output['timestamp'] = self.workflow['timestamp'].tolist()\n",
    "        output['bytesRead'] = self.workflow['bytesRead'].tolist()\n",
    "        output['bytesWritten'] = self.workflow['bytesWritten'].tolist()\n",
    "        return output\n",
    "    \n",
    "    def plot_workflow(self):\n",
    "        \"\"\"\n",
    "        Plot the time series of bytesRead and bytesWritten for the synthesized workflow.\n",
    "        \"\"\"\n",
    "        plt.figure(figsize=(14, 6))\n",
    "        plt.plot(self.workflow['timestamp'], self.workflow['bytesRead'], label='Bytes Read')\n",
    "        plt.plot(self.workflow['timestamp'], self.workflow['bytesWritten'], label='Bytes Written')\n",
    "        plt.xlabel('Timestamp')\n",
    "        plt.ylabel('Bytes')\n",
    "        plt.title('Bytes Read and Written Over Time')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "\n",
    "# Example usage\n",
    "data_for_jobs = {\n",
    "    'job1': pd.DataFrame({'timestamp': [1687960785000, 1687960790000, 1687960795000], 'bytesRead': [10, 20, 30], 'bytesWritten': [5, 15, 25]}),\n",
    "    'job2': pd.DataFrame({'timestamp': [1687961145000, 1687961150000, 1687961155000], 'bytesRead': [40, 50, 60], 'bytesWritten': [35, 45, 55]})\n",
    "}\n",
    "\n",
    "synthesizer = WorkflowSynthesizer()\n",
    "synthesizer.synthetize(data_for_jobs)\n",
    "print(synthesizer.workflow)\n",
    "print(synthesizer.to_dict())\n",
    "#synthesizer.plot_workflow()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For accessPatternRead.json, the columns are: ['timestamp', 'access_pattern_read_random', 'access_pattern_read_sequential', 'access_pattern_read_stride', 'access_pattern_read_unclassified']\n",
      "For ioSizesWrite.json, the columns are: ['timestamp', 'io_sizes_write_0B_16B', 'io_sizes_write_16B_4KB', 'io_sizes_write_4KB_128KB', 'io_sizes_write_128KB_1MB', 'io_sizes_write_1MB_16MB', 'io_sizes_write_16MB_128MB', 'io_sizes_write_128MB_+']\n",
      "For operationsCount.json, the columns are: ['timestamp', 'operations_count_read', 'operations_count_write']\n",
      "For volume.json, the columns are: ['timestamp', 'bytesRead', 'bytesWritten']\n"
     ]
    }
   ],
   "source": [
    "def camel_case_to_snake_case(name):\n",
    "    \"\"\"\n",
    "    Convert a string from CamelCase to snake_case.\n",
    "    \n",
    "    Parameters:\n",
    "        name (str): The string in CamelCase.\n",
    "        \n",
    "    Returns:\n",
    "        str: The string in snake_case.\n",
    "    \"\"\"\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)).lower()\n",
    "\n",
    "# Update the get_column_names function to include the renamed columns using the new prefix strategy\n",
    "def get_column_names(json_file_name):\n",
    "    \"\"\"\n",
    "    Given a JSON file name, this function returns a list of relevant column names.\n",
    "    \n",
    "    Parameters:\n",
    "        json_file_name (str): The name of the JSON file.\n",
    "        \n",
    "    Returns:\n",
    "        list: A list of strings representing column names.\n",
    "    \"\"\"\n",
    "    # Extract the prefix from the file name and convert it to snake_case\n",
    "    prefix = camel_case_to_snake_case(json_file_name.split('.')[0])\n",
    "    \n",
    "    column_name_mapping = {\n",
    "        \"accessPatternRead.json\": [\"timestamp\", f\"{prefix}_random\", f\"{prefix}_sequential\", f\"{prefix}_stride\", f\"{prefix}_unclassified\"],\n",
    "        \"accessPatternWrite.json\": [\"timestamp\", f\"{prefix}_random\", f\"{prefix}_sequential\", f\"{prefix}_stride\", f\"{prefix}_unclassified\"],\n",
    "        \"ioSizesRead.json\": [\"timestamp\", f\"{prefix}_0B_16B\", f\"{prefix}_16B_4KB\", f\"{prefix}_4KB_128KB\", f\"{prefix}_128KB_1MB\", f\"{prefix}_1MB_16MB\", f\"{prefix}_16MB_128MB\", f\"{prefix}_128MB_+\"],\n",
    "        \"ioSizesWrite.json\": [\"timestamp\", f\"{prefix}_0B_16B\", f\"{prefix}_16B_4KB\", f\"{prefix}_4KB_128KB\", f\"{prefix}_128KB_1MB\", f\"{prefix}_1MB_16MB\", f\"{prefix}_16MB_128MB\", f\"{prefix}_128MB_+\"],\n",
    "        \"operationsCount.json\": [\"timestamp\", f\"{prefix}_read\", f\"{prefix}_write\"],\n",
    "        \"volume.json\": [\"timestamp\", \"bytesRead\", \"bytesWritten\"]\n",
    "    }\n",
    "    \n",
    "    return column_name_mapping.get(json_file_name, [])\n",
    "\n",
    "# Test the above function\n",
    "test_file_names = [\"accessPatternRead.json\", \"ioSizesWrite.json\", \n",
    "                   \"operationsCount.json\", \"volume.json\"]\n",
    "for name in test_file_names:\n",
    "    print(f\"For {name}, the columns are: {get_column_names(name)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "def is_file_extension(filename, expected_extension):\n",
    "    \"\"\"\n",
    "    Check if the file has the expected extension.\n",
    "    \n",
    "    Parameters:\n",
    "        filename (str): The name of the file.\n",
    "        expected_extension (str): The expected file extension (without the dot).\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if the file has the expected extension, False otherwise.\n",
    "    \"\"\"\n",
    "    _, file_extension = os.path.splitext(filename)\n",
    "    \n",
    "    return file_extension == f\".{expected_extension}\"\n",
    "\n",
    "# Tests\n",
    "assert is_file_extension(\"example.json\", \"json\") == True\n",
    "assert is_file_extension(\"example.txt\", \"json\") == False\n",
    "assert is_file_extension(\"example.json\", \"txt\") == False\n",
    "assert is_file_extension(\"example.JSON\", \"json\") == False  # Case-sensitive\n",
    "assert is_file_extension(\"example\", \"json\") == False       # No extension\n",
    "\n",
    "print(\"All tests passed!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " Processing workflow : ECMWF-649c3c40cc9340246f87cb58...\n",
      "Found job_folder: 371912, is_folder:True\n",
      "job_folder: 371912\n",
      "    Browsing 371912...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371912.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371912.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371912.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371912.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371912.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371912.csv\n",
      "Found job_folder: 371913, is_folder:True\n",
      "job_folder: 371913\n",
      "    Browsing 371913...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371913.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371913.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371913.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371913.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371913.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371913.csv\n",
      "Found job_folder: 371911, is_folder:True\n",
      "job_folder: 371911\n",
      "    Browsing 371911...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371911.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371911.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371911.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371911.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371911.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371911.csv\n",
      "Found job_folder: 371906, is_folder:True\n",
      "job_folder: 371906\n",
      "    Browsing 371906...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371906.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371906.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371906.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371906.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371906.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371906.csv\n",
      "Found job_folder: 371924, is_folder:True\n",
      "job_folder: 371924\n",
      "    Browsing 371924...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371924.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371924.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371924.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371924.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371924.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371924.csv\n",
      "Found job_folder: 371910, is_folder:True\n",
      "job_folder: 371910\n",
      "    Browsing 371910...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371910.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371910.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371910.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371910.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371910.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371910.csv\n",
      "Found job_folder: 371903, is_folder:True\n",
      "job_folder: 371903\n",
      "    Browsing 371903...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371903.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371903.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371903.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371903.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371903.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371903.csv\n",
      "Found job_folder: 371902, is_folder:True\n",
      "job_folder: 371902\n",
      "    Browsing 371902...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371902.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371902.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371902.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371902.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371902.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371902.csv\n",
      "Found job_folder: 371900, is_folder:True\n",
      "job_folder: 371900\n",
      "    Browsing 371900...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371900.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371900.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371900.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371900.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371900.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371900.csv\n",
      "Found job_folder: 371908, is_folder:True\n",
      "job_folder: 371908\n",
      "    Browsing 371908...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371908.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371908.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371908.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371908.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371908.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371908.csv\n",
      "Found job_folder: 371905, is_folder:True\n",
      "job_folder: 371905\n",
      "    Browsing 371905...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371905.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371905.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371905.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371905.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371905.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371905.csv\n",
      "Found job_folder: 371907, is_folder:True\n",
      "job_folder: 371907\n",
      "    Browsing 371907...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371907.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371907.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371907.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371907.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371907.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371907.csv\n",
      "Found job_folder: 371904, is_folder:True\n",
      "job_folder: 371904\n",
      "    Browsing 371904...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371904.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371904.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371904.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371904.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371904.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371904.csv\n",
      "Found job_folder: 371909, is_folder:True\n",
      "job_folder: 371909\n",
      "    Browsing 371909...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 371909.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 371909.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 371909.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 371909.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 371909.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 371909.csv\n",
      "----\n",
      " Processing workflow : LQCD-64873bafcc9340246f412faf...\n",
      "Found job_folder: 367660, is_folder:True\n",
      "job_folder: 367660\n",
      "    Browsing 367660...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 367660.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 367660.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 367660.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 367660.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 367660.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 367660.csv\n",
      "Found job_folder: 367661, is_folder:True\n",
      "job_folder: 367661\n",
      "    Browsing 367661...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 367661.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 367661.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 367661.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 367661.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 367661.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 367661.csv\n",
      "Found job_folder: 367665, is_folder:True\n",
      "job_folder: 367665\n",
      "    Browsing 367665...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 367665.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 367665.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 367665.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 367665.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 367665.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 367665.csv\n",
      "Found job_folder: 367668, is_folder:True\n",
      "job_folder: 367668\n",
      "    Browsing 367668...\n",
      "      Processing ioSizesWrite.json...\n",
      "            Saving here... 367668.csv\n",
      "      Processing volume.json...\n",
      "            Saving here... 367668.csv\n",
      "      Processing ioSizesRead.json...\n",
      "            Saving here... 367668.csv\n",
      "      Processing operationsCount.json...\n",
      "            Saving here... 367668.csv\n",
      "      Processing accessPatternWrite.json...\n",
      "            Saving here... 367668.csv\n",
      "      Processing accessPatternRead.json...\n",
      "            Saving here... 367668.csv\n",
      "----\n",
      " Processing workflow : deep_jobs.ipynb...\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty dictionary to hold the data for each job\n",
    "data_for_jobs = {}\n",
    "\n",
    "# Loop through all items in the current directory\n",
    "for wf_folder in os.listdir():\n",
    "    # Check if the item is a folder and if its name is numeric (i.e., a job number)\n",
    "    print(f\"----\\nProcessing workflow : {wf_folder}...\")\n",
    "    if os.path.isdir(wf_folder):\n",
    "        # Initialize a dictionary to hold the data for this specific job\n",
    "        data_for_this_job = {}\n",
    "        # Loop through all JSON files in this folder\n",
    "        for job_folder in os.listdir(wf_folder):\n",
    "            is_folder = os.path.isdir(os.path.join(os.getcwd(), wf_folder, job_folder))\n",
    "            print(f\"Found job_folder: {job_folder}, is_folder:{is_folder}\")\n",
    "            # Ensure that we are dealing with job folder\n",
    "            if is_folder:\n",
    "                print(f\"job_folder: {job_folder}\")\n",
    "                # Initialize a DataFrame to hold the data for this specific job\n",
    "                df_for_this_job = pd.DataFrame()\n",
    "                print(f\"    Browsing {job_folder}...\")\n",
    "                for json_file in os.listdir(os.path.join(wf_folder, job_folder)):\n",
    "                    if is_file_extension(json_file, \"json\"):\n",
    "                        print(f\"      Processing {json_file}...\")\n",
    "                        # Construct the full path to the JSON file\n",
    "                        json_file_path = os.path.join(os.getcwd(), wf_folder, job_folder, json_file)\n",
    "                        \n",
    "                        # Read the JSON file into a list of lists\n",
    "                        with open(json_file_path, 'r') as f:\n",
    "                            json_data = json.load(f)\n",
    "                        \n",
    "                        # Create a temporary DataFrame from the JSON data\n",
    "                        df_temp = pd.DataFrame(json_data, columns=\n",
    "                                            get_column_names(json_file))\n",
    "                        \n",
    "                        # Merge the temporary DataFrame into the DataFrame for this job, based on the 'timestamp' column\n",
    "                        if df_for_this_job.empty:\n",
    "                            df_for_this_job = df_temp\n",
    "                        else:\n",
    "                            df_for_this_job = pd.merge(df_for_this_job, \n",
    "                                                       df_temp, \n",
    "                                                       on='timestamp', \n",
    "                                                       how='outer')\n",
    "                            \n",
    "                    # Save the DataFrame for this job to a CSV file\n",
    "                    csv_file_path = os.path.join(os.getcwd(), wf_folder, f\"{job_folder}.csv\")\n",
    "                    \n",
    "                    #df_for_this_job.to_csv(csv_file_path, index=False)\n",
    "                    print(f\"            Saving here... {job_folder}.csv\")\n",
    "                    \n",
    "                    # Add this DataFrame to the dictionary\n",
    "                    data_for_jobs[job_folder] = df_for_this_job\n",
    "\n",
    "            else:\n",
    "                print(f\"Skipping {job_folder}...\")\n",
    "# synthesizer = WorkflowSynthesizer()\n",
    "# synthesizer.synthetize(data_for_jobs)\n",
    "# print(synthesizer.workflow)\n",
    "# print(synthesizer.to_dict())\n",
    "# synthesizer.plot_workflow()\n",
    "\n",
    "\n",
    "def list_and_classify_directory_contents(directory_path):\n",
    "    \"\"\"\n",
    "    List and classify the contents of a given directory into folders and files.\n",
    "    \n",
    "    Parameters:\n",
    "        directory_path (str): The path to the directory to list.\n",
    "        \n",
    "    Returns:\n",
    "        None: Prints the classification results.\n",
    "    \"\"\"\n",
    "    for item in os.listdir(directory_path):\n",
    "        item_path = os.path.join(directory_path, item)\n",
    "        \n",
    "        if os.path.isdir(item_path):\n",
    "            print(f\"{item} -> Folder\")\n",
    "        elif os.path.isfile(item_path):\n",
    "            print(f\"{item} -> File\")\n",
    "        else:\n",
    "            print(f\"{item} -> Unknown\")\n",
    "\n",
    "# Usage example\n",
    "# Replace 'your_directory_path_here' with the path of the directory you want to list and classify.\n",
    "# Uncomment the line below to run the function.\n",
    "# list_and_classify_directory_contents('your_directory_path_here')\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iosea-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
